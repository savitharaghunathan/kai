[postgresql]
host = "127.0.0.1"
database = "kai"
user = "kai"
password = "dog8code"

[models]
# How to run with: (look at model_provider.py for more info)
#   IBM served granite
#     provider = "IBMGranite"
#     args = { model_id = "ibm/granite-13b-chat-v2" }
#   IBM served mistral
#     provider = "IBMOpenSource"
#     args = { model_id = "ibm-mistralai/mixtral-8x7b-instruct-v01-q" }
#   IBM served codellama
#     provider = "IBMOpenSource"
#     args = { model_id = "meta-llama/llama-2-13b-chat" }
#   OpenAI GPT 3.5
#     provider = "OpenAI"
#     args = { model_id = "gpt-4" }
#   OpenAI GPT 4
#     provider = "OpenAI"
#     args = { model_id = "gpt-3.5-turbo" }
#   provider = "IBMGranite"
#   args = { model_id = "ibm/granite-13b-chat-v2" }

provider = "IBMOpenSource"
args = { model_id = "mistralai/mixtral-8x7b-instruct-v0-1" }
#args = { model_id = "ibm-mistralai/mixtral-8x7b-instruct-v01-q" }


# Most models also support these adddiontal keys in `args`:
#     temperature (default is 0.1)
#     top_k (default is 50)
#     top_p (default is 1)
#     max_new_tokens (default depends on model)
#     min_new_tokens (default depends on model)

# Here for later, we want to be able to configure which embeddings are used when
# we start to integrate them into the project
[embeddings]
todo = true
